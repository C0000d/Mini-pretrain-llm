import warnings
warnings.filterwarnings("ignore")
import datasets
import os
import requests

import heapq
import re
import urllib
import fasttext

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------
def paragraph_length_filter(x):
    """Return false if a page has too few lines or lines are too short"""
    lines = x['text'].split('\n')  # get lines of each file(x)
    if (
        len(lines) < 3             # check if the lines contains more than 3 chars
        or min(heapq.nlargest(3, [len(line) for line in lines])) < 3 # is the shortest line among 3 longest of all lines shorter than 3 chars
    ):
        return False
    return True

def find_duplicates(paragraphs):
    """
    Use this function to help find number of duplications in the paragraphs
    :param paragraphs:
    :return: [duplicate_elements, duplicate_chars]
    - duplicate_elements: how many *paragraph strings* are duplicates
    - duplicate_chars: total character count in those duplicates
    """
    unique_x = set()
    duplicate_chars = 0
    duplicate_elements = 0
    for element in paragraphs:
        if element in unique_x:
            duplicate_chars += len(element)
            duplicate_elements += 1
        else:
            unique_x.add(element)
    return duplicate_elements, duplicate_chars

def paragraph_repetition_filter(x):
    """
    Returns False if a text a has too many dupliated paragraphs
    :param x: each row of the dataset
    :return: True/False
    """
    text = x['text']
    paragraphs = re.compile(r"\n{2, }").split(text.strip())  # split the text into paragraphs (newlines at the beginning and the end already removed by strip()) if there's more than 2 newlines
    paragraphs_duplicates, char_duplicates = find_duplicates(paragraphs)
    if paragraphs_duplicates / len(paragraphs) > 0.3: # len(paragraphs): num of paragraphs within the text
        return False
    if char_duplicates / len(text) > 0.2:  # len(text): num of characters within the text
        return False
    return True

def deduplication(ds):
    def dedup_func(x):
        """use this function to remove duplicate entries"""
        if x['text'] in unique_text:
            return False            # remove duplicate rows
        else:
            unique_text.add(x['text'])
            return True

    unique_text = set()

    ds = ds.filter(dedup_func, load_from_cache_file=False, num_proc=1) # just 1 process, no parallelism, no concurrency
    return ds

def english_language_filter(ds):
    # load the language detection model
    model = fasttext.load_model('./models/L2_language_model.bin')  # load the FastText model trained by upstage

    def is_english(x):
        # predict language of the text and probability
        language, score = model.predict(x['text'].replace('\n', ''))

        language = language[0].split("__")[2]
        return score > 0.4 and language == "en"

    ds = ds.filter(is_english, load_from_cache_file=False, num_proc=1)
    return ds


# ---------------------------------------------------------------------------
# Data preparation
# ---------------------------------------------------------------------------
# 1. Load dataset from Hugging Face
pretraining_dataset = datasets.load_dataset(
    "upstage/Pretraining_Dataset",  # subset of red pajama
    split="train"
)

# only get the text column
pretraining_dataset = pretraining_dataset.select_columns(["text"]) # return a full dataset object
# pretraining_dataset["text"] will only return a raw list of text

# compare against fine-tuning datasets
instruction_dataset = datasets.load_dataset(
    "c-s-ale/alpaca-gpt4-data",   # Alpaca-style training, generated by GPT4, meant for fine-yuning otehr models to behave like GPT-4
    split='train'
)
print(instruction_dataset)

i=0
print("Instruction: " + instruction_dataset[i]["instruction"]
      + "\nInput: " + instruction_dataset[i]["input"]
      + "\nOutput: " + instruction_dataset[i]["output"])

# 2. Pack open-sourced data
# here I scraped python script and translate them into hugging face dataset object
code_dir = "./code_dataset"  # path to store python scripts, should be existed
os.makedirs(code_dir, exist_ok=True)  # make the directory if not exist, if exist the program quietly keep going
urls = [
    "https://raw.githubusercontent.com/TheAlgorithms/Python/master/searches/double_linear_search_recursion.py",
    "https://raw.githubusercontent.com/KosingZhu/tensorflow/master/tensorflow/python/tools/module_util.py",  #
    "https://raw.githubusercontent.com/EricRemmerswaal/tensorflow/master/tensorflow/python/distribute/distribute_coordinator_context.py",  #
    "https://raw.githubusercontent.com/computationalartist/tensorflow/master/tensorflow/python/ops/numpy_ops/integration_test/benchmarks/numpy_mlp.py",
    "https://raw.githubusercontent.com/Van-an/tensorflow/master/tensorflow/python/distribute/coordinator/values.py",
    "https://raw.githubusercontent.com/nkgwer/tensorflow/master/tensorflow/lite/tools/visualize.py",
    "https://raw.githubusercontent.com/gitblazer/youtube-dl/master/youtube_dl/version.py",
    "https://raw.githubusercontent.com/Joshua-Barawa/My-Photos/master/venv/lib/python3.8/site-packages/django/contrib/messages/__init__.py",
    "https://raw.githubusercontent.com/PaliC/pytorch/master/test/fx/test_subgraph_rewriter.py"
]

# read into these urls, write file to local directory
for url in urls:
    print(f"Working on url: {url}")
    response = requests.get(url)   # sends an HTTP request to the URL, the response includes: .status_code, .content
    file_name = os.path.basename(url)   # extracts the filename from the url
    file_path = os.path.join(code_dir, file_name) # the path we want the file to store locally

    with open(file_path, "wb") as file:
        file.write(response.content)

files = os.listdir(code_dir)
for file in files:
    print(file)

# group these codes into a list of dictionary
code_dataset = []
for file in os.listdir(code_dir):
    code_dataset.append(
        {'text': open(os.path.join(code_dir, file), 'r').read()}
    )

# convert list to hugging face Dataset object
code_dataset = datasets.Dataset.from_list(code_dataset)  # list will record the order, otheroption: from dict, from pandas
print(code_dataset)

# combine the downloaded dataset with one python code dataset
dataset = datasets.concatenate_datasets(
    [pretraining_dataset, code_dataset]
)  # concatenate_dataset expects a list of datasets
print(dataset)

# ---------------------------------------------------------------------------
# Data cleaning
# ---------------------------------------------------------------------------
print("dataset.num_rows", dataset.num_rows)

# remove examples that are too short
dataset = dataset.filter(           # keeps only the rows for which fn(paragraph_length_filter in this case) return true
    paragraph_length_filter,        # fn is applied to every row, unless batched=True, then it's applied to every batch
    load_from_cache_file=False      # forces the filter to run again even if a cached Arrow file exists.
)                                   # return a new dataset object
print("after length filter", dataset.num_rows)

# Remove files with too many repeated texts
dataset = dataset.filter(
    paragraph_repetition_filter,
    load_from_cache_file=False
)
print("after repetition filter", dataset.num_rows)

# remove repeated text within training examples
dataset = deduplication(dataset)
print("after duplication filter", dataset.num_rows)

# Quality filter - Language
dataset = english_language_filter(dataset)
print("after english language filter", dataset.num_rows)

# save the dataset to disk
file_path = "./data/preprocessed_dataset.parquet"
dataset.to_parquet(file_path)

